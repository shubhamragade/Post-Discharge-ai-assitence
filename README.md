
# ğŸ¥ **Post-Discharge Medical AI Assistant**

## ğŸ“˜ **Overview**

**Post-Discharge Medical AI Assistant** is an intelligent **multi-agent healthcare system** designed to support patients after hospital discharge.  
It leverages **Retrieval-Augmented Generation (RAG)**, **LLM reasoning**, and **real-time medical web search** to provide **accurate, grounded, and context-aware medical responses**.

---

## ğŸ’¡ **Core Capabilities**

- âœ… Patient lookup & verification  
- âœ… Context-aware medical responses (RAG)  
- âœ… Real-time medical literature web search  
- âœ… Multi-agent dialogue (Receptionist + Clinical AI)  
- âœ… Interactive Streamlit chat interface  

---

## ğŸ§© **System Architecture**


flowchart TD
A[ğŸ‘¤ Patient] -->|Query| B[ğŸ¤– Receptionist Agent]
B -->|Medical Query Detected| C[ğŸ©º Clinical Agent]
C -->|RAG Context Retrieval| D[(ğŸ“š ChromaDB - Nephrology PDF)]
C -->|If Not Found| E[ğŸŒ Web Search (DuckDuckGo)]
C -->|Response| F[ğŸ’¬ Streamlit Chat UI]


âš™ï¸ Tech Stack
LayerTechnologyFrontendStreamlitBackendFastAPILLMGroq API (Llama-3.1-8b-instant)RAG EngineLangChain + ChromaDBEmbeddingssentence-transformers/all-MiniLM-L6-v2Web Searchddgs (DuckDuckGo Search)Data SourceComprehensive Clinical Nephrology (PDF)LoggingPython logging + JSON
ğŸ§  LLM Selection
Model: llama-3.1-8b-instant (via Groq API)
Why this model?

* ğŸš€ Lightweight & ultra-fast inference

* ğŸ§© Ideal for retrieval-based medical reasoning

* ğŸ’° Free API access and low latency

ğŸ§® Vector Database Selection
Database: ChromaDB
Why ChromaDB?

* âš¡ Lightweight, local, and open-source

* ğŸ”— Integrates seamlessly with LangChain

* ğŸ’¾ Supports persistent offline vector storage

Embeddings Model: sentence-transformers/all-MiniLM-L6-v2
ğŸ§© RAG Implementation Pipeline

1. Text Splitting â†’ Chunk PDF text

2. Embedding Generation â†’ Encode using MiniLM model

3. Vector Storage â†’ Save in backend/data/chroma_db/

4. Retrieval â†’ Fetch top-k relevant chunks

5. LLM Fusion â†’ Combine retrieved context + query

6. Response Generation â†’ Produce grounded answers with citations

ğŸŒ Web Search Integration
When RAG confidence is low or the user requests recent information,\
the Clinical Agent triggers DuckDuckGo Search (ddgs).
Example:
ğŸ‘¤ Patient: â€œWhat's the latest research on SGLT2 inhibitors for kidney disease?â€\
ğŸ¤– Clinical Agent: â€œThis requires recent information. Let me search for you...\
According to recent studies, SGLT2 inhibitors significantly reduce CKD progression and cardiovascular mortality.â€\
ğŸ“š Sources: Nature 2025, PubMed 2024, NEJM 2025
ğŸ§¬ Multi-Agent Framework
AgentResponsibilityReceptionist AgentGreets patient, verifies ID, routes medical queriesClinical AgentAnswers using RAG + Web SearchAgent ManagerControls flow between agents and maintains state
ğŸ§¾ Logging & Traceability
All agent interactions are recorded for auditing and debugging.
ğŸ“‚ Location: backend/logs/chat_memory.json
Sample Log:

{
  "timestamp": "2025-11-02T10:01:00",
  "agent": "clinical",
  "event": "RAG_response",
  "query": "I'm having swelling in my legs",
  "response": "Based on CKD guidelines..."
}

ğŸ§  Patient Data Retrieval

* Stored in patients_data.json

* Receptionist Agent retrieves patient details (diagnosis, meds, follow-up info)

* Context is passed to Clinical Agent for response generation

ğŸ—ï¸ Project Structure


post_discharge_medical_ai_poc/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ receptionist_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ clinical_agent.py
â”‚   â”‚   â”‚   â””â”€â”€ agent_manager.py
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â”‚   â””â”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ patient_data_tool.py
â”‚   â”‚   â”œâ”€â”€ llm_response_handler.py
â”‚   â”‚   â”œâ”€â”€ logging_config.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ patients_data.json
â”‚       â””â”€â”€ nephrology_refs/
â”‚           â””â”€â”€ comprehensive-clinical-nephrology.pdf
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â””â”€â”€ logo.png
â”‚   â””â”€â”€ components/
â”‚       â””â”€â”€ chat_ui.py
â”‚
â””â”€â”€ README.md


ğŸ’¬ Example Interaction
ğŸ‘¤ Patient: â€œI'm having swelling in my legs. Should I be worried?â€\
ğŸ¤– Receptionist Agent: â€œThis sounds like a medical concern. Let me connect you with our Clinical AI Agent.â€\
ğŸ©º Clinical Agent (RAG): â€œBased on your CKD diagnosis and nephrology guidelines, leg swelling may indicate fluid retention.\
ğŸ“š [Citations: p.614, p.905]â€
ğŸ‘¤ Patient: â€œWhat's the latest research on SGLT2 inhibitors for kidney disease?â€\
ğŸ©º Clinical Agent (Web Search): â€œThis requires recent information. Let me search for you...\
ğŸ“– [Results from PubMed 2025, NEJM 2024, Nature 2025]â€
ğŸš€ Running the Project
1ï¸âƒ£ Create Virtual Environment


python -m venv .venv
.\.venv\Scripts\activate   # (Windows)
source .venv/bin/activate  # (Linux/Mac)


2ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

3ï¸âƒ£ Start Backend (FastAPI)

```
uvicorn backend.app.main:app --reload
```

4ï¸âƒ£ Start Frontend (Streamlit)

```
streamlit run frontend/streamlit_app.py
```

ğŸ” Environment Variables (.env)

```
GROQ_API_KEY=your_groq_api_key_here
CHROMA_PATH=backend/data/chroma_db
```

ğŸ§± System Workflow
1ï¸âƒ£ Receptionist Agent receives user query\
2ï¸âƒ£ If medical â†’ escalates to Clinical Agent\
3ï¸âƒ£ Clinical Agent performs RAG retrieval\
4ï¸âƒ£ If context unavailable â†’ triggers Web Search\
5ï¸âƒ£ Returns grounded response with citations\
6ï¸âƒ£ Logs interaction in JSON
ğŸ“¦ Requirements

```
fastapi
uvicorn
streamlit
langchain
langchain-community
langchain-chroma
langchain-huggingface
chromadb
sentence-transformers
ddgs
groq
pydantic
Pillow
python-dotenv
```

ğŸ”® Future Enhancements

* ğŸ”— EHR / FHIR integration

* ğŸ“Š Confidence-based RAG response scoring

* ğŸ§  Explainable AI reasoning layer

* ğŸ—£ï¸ Voice-enabled interaction

* ğŸ³ Dockerized deployment

ğŸ‘¨â€ğŸ’» Contributor
Shubham Ragade\
AI Developer | DataSmith AI (GenAI Internship 2025)\
ğŸ“§ shubhamragade.ai@gmail.com
â­ If you find this project useful, consider giving it a star on GitHub! â­

````

---

## ğŸ§© 2. `requirements.txt`

```text
fastapi
uvicorn
streamlit
langchain
langchain-community
langchain-chroma
langchain-huggingface
chromadb
sentence-transformers
ddgs
groq
pydantic
Pillow
python-dotenv
````

ğŸ” 3. .env.example

```
# === LLM Configuration ===
GROQ_API_KEY=your_groq_api_key_here

# === Vector Database Path ===
CHROMA_PATH=backend/data/chroma_db

# === App Configuration ===
LOG_PATH=backend/logs/chat_memory.json
PATIENT_DATA_PATH=backend/data/patients_data.json
NEPHROLOGY_PDF_PATH=backend/data/nephrology_refs/comprehensive-clinical-nephrology.pdf
```

ğŸ“¦ Copy Instructions
1ï¸âƒ£ Create your folder â†’ post_discharge_medical_ai_poc/\
2ï¸âƒ£ Inside it, make these files:

# 1. Clone & Enter
git clone https://github.com/shubhamragade/Post-Discharge-ai-assitence.git
cd post_discharge_medical_ai_poc

# 2. Virtual Env
python -m venv .venv && source .venv/bin/activate   # mac/linux
# .\ .venv\Scripts\activate   # windows

# 3. Install
pip install -r requirements.txt

# 4. Add your key
cp .env.example .env
# â†’ paste GROQ_API_KEY=gho_...

# 5. Launch
# Terminal 1
uvicorn backend.app.main:app --reload

# Terminal 2
streamlit run frontend/streamlit_app.py

